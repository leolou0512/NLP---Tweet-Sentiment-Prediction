{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f279ea5-2755-420f-be4f-63d6a8f71e67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (2.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from datasets) (2.28.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: dill<0.3.6 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (0.8.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (8.0.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "1.13.1+cu116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (3.4.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (8.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from spacy) (1.9.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.4.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from jinja2->spacy) (2.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "     --------------------------------------- 12.8/12.8 MB 38.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from en-core-web-sm==3.4.1) (3.4.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.9.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.28.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (65.6.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (23.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.6.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.14)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.2)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: symspellpy in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (6.7.7)\n",
      "Requirement already satisfied: editdistpy>=0.1.3 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from symspellpy) (0.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gingerit in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (0.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.25.1 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from gingerit) (2.28.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from requests<3.0.0,>=2.25.1->gingerit) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from requests<3.0.0,>=2.25.1->gingerit) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from requests<3.0.0,>=2.25.1->gingerit) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from requests<3.0.0,>=2.25.1->gingerit) (1.26.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emot in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# datasets for BERT\n",
    "!pip install datasets\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "# relevant packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "torch.cuda.empty_cache()\n",
    "from tqdm.notebook import tqdm\n",
    "# random check \n",
    "print(torch.__version__)\n",
    "\n",
    "df = pd.read_csv(\"tweet_emotions.csv\")\n",
    "\n",
    "# Install spaCy (run in terminal/prompt)\n",
    "import sys\n",
    "!{sys.executable} -m pip install spacy\n",
    "\n",
    "# Download spaCy's  'en' Model\n",
    "!{sys.executable} -m spacy download en_core_web_sm\n",
    "\n",
    "!pip install -U symspellpy\n",
    "\n",
    "#for spell and slang correction\n",
    "!pip install gingerit\n",
    "from gingerit.gingerit import GingerIt\n",
    "\n",
    "#for emoticons\n",
    "!pip install emot --upgrade\n",
    "import emot \n",
    "emot_obj = emot.core.emot() \n",
    "\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "import pkg_resources\n",
    "import re, string, json\n",
    "import spacy\n",
    "from datasets import Value, ClassLabel, Features, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c14eb2e-22da-449c-b262-4019ea8999a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral      8638\n",
       "worry        8459\n",
       "happiness    5209\n",
       "sadness      5165\n",
       "love         3842\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we want to remove small datasets \n",
    "# df = df[~df.sentiment.str.contains('\\|')]   \n",
    "df = df[df.sentiment != 'anger'] #& 'boredom' & 'enthusiasm' & 'empty'\n",
    "df = df[df.sentiment != 'boredom']\n",
    "df = df[df.sentiment != 'enthusiasm']\n",
    "df = df[df.sentiment != 'empty']\n",
    "df = df[df.sentiment != 'sentiment'] #there is sentiment in sentiments!\n",
    "df = df[df.sentiment != 'fun']\n",
    "df = df[df.sentiment != 'relief']\n",
    "df = df[df.sentiment != 'hate']\n",
    "df = df[df.sentiment != 'surprise']\n",
    "\n",
    "df.sentiment.value_counts()\n",
    "#class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81165a6b-c599-4ff8-98c0-8b1eea8bcba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sadness': 0, 'neutral': 1, 'worry': 2, 'love': 3, 'happiness': 4}\n"
     ]
    }
   ],
   "source": [
    "#build dictionary, key: emotion, value: \n",
    "possible_labels = df.sentiment.unique()\n",
    "label_dict = {}\n",
    "#loop over index\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ade3b52-25c4-4d1e-a849-a38219bc8fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Value, ClassLabel, Features, Dataset\n",
    "\n",
    "#build new column for these values\n",
    "df[\"label\"] = df.sentiment.replace(label_dict)\n",
    "\n",
    "features = Features({\"label\": Value(\"int64\"), \"label\": ClassLabel(num_classes=5, names=[0,1,2,3,4])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b635879-50cf-46a3-89dc-3701e5113b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>0</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>0</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>1</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1956968477</td>\n",
       "      <td>worry</td>\n",
       "      <td>Re-pinging @ghostridah14: why didn't you go to...</td>\n",
       "      <td>2</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1956968487</td>\n",
       "      <td>sadness</td>\n",
       "      <td>I should be sleep, but im not! thinking about ...</td>\n",
       "      <td>0</td>\n",
       "      <td>not_set</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id sentiment                                            content  \\\n",
       "1  1956967666   sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2  1956967696   sadness                Funeral ceremony...gloomy friday...   \n",
       "4  1956968416   neutral  @dannycastillo We want to trade with someone w...   \n",
       "5  1956968477     worry  Re-pinging @ghostridah14: why didn't you go to...   \n",
       "6  1956968487   sadness  I should be sleep, but im not! thinking about ...   \n",
       "\n",
       "   label data_type  \n",
       "1      0   not_set  \n",
       "2      0   not_set  \n",
       "4      1   not_set  \n",
       "5      2   not_set  \n",
       "6      0   not_set  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#stratified split\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.index.values,\n",
    "                                                 df.label.values,\n",
    "                                                 test_size = 0.025,\n",
    "                                                 stratify = df.label.values\n",
    "                                                 )\n",
    "\n",
    "df['data_type'] = ['not_set']*df.shape[0]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b3f4323-8448-404d-b80c-079c081b6f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">happiness</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>train</th>\n",
       "      <td>5079</td>\n",
       "      <td>5079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>130</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">love</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>train</th>\n",
       "      <td>3746</td>\n",
       "      <td>3746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">neutral</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>8422</td>\n",
       "      <td>8422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>216</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">sadness</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>train</th>\n",
       "      <td>5036</td>\n",
       "      <td>5036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">worry</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>8247</td>\n",
       "      <td>8247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           tweet_id  content\n",
       "sentiment label data_type                   \n",
       "happiness 4     train          5079     5079\n",
       "                val             130      130\n",
       "love      3     train          3746     3746\n",
       "                val              96       96\n",
       "neutral   1     train          8422     8422\n",
       "                val             216      216\n",
       "sadness   0     train          5036     5036\n",
       "                val             129      129\n",
       "worry     2     train          8247     8247\n",
       "                val             212      212"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[X_train, 'data_type'] = 'train'\n",
    "df.loc[X_val, 'data_type'] = 'val'\n",
    "\n",
    "df.groupby(['sentiment', 'label', 'data_type']).count()\n",
    "#group by using count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16d8df25-cbe5-4354-8170-245a69a18207",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalization_pipeline(sentences):\n",
    "    print(\"##############################\")\n",
    "    print(\"Starting Normalization Process\")\n",
    "    sentences = _simplify_punctuation_and_whitespace(sentences) # !!!!! \"      \"\n",
    "    sentences = _normalize_contractions(sentences) #also corrects spelling now\n",
    "    print(\"Normalization Process Finished\")\n",
    "    print(\"##############################\")\n",
    "    return sentences\n",
    "\n",
    "    \n",
    "def _simplify_punctuation_and_whitespace(sentence_list):\n",
    "    \"\"\"\n",
    "    words with more than 4 all-capital words will get <-EMPW \n",
    "    \"\"\"\n",
    "    norm_sents = []\n",
    "    print(\"Replacing -URL- , Replacing @MENTION and #HASHTAG, Reducing character repetitions, \")\n",
    "    print(\"Simplifying punctuation, Removing whitespaces\")\n",
    "\n",
    "    for sentence in tqdm(sentence_list):\n",
    "        sent = _replace_urls(sentence)\n",
    "        sent = _mention_hash(sent)\n",
    "        sent = _simplify_punctuation(sent)\n",
    "        sent = _reduce_repetitions(sent)\n",
    "        sent = _normalize_whitespace(sent)\n",
    "        norm_sents.append(sent)\n",
    "    return norm_sents\n",
    "\n",
    "\n",
    "def _replace_urls(text):\n",
    "    url_regex = r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
    "    text = re.sub(url_regex, \"-URL-\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def _mention_hash(in_str):\n",
    "    \"\"\"\n",
    "     @MENTIONs and #HASHTAGs will take forms of @men and #has \n",
    "    note: BEWARE OF USES OF # AND @ AND SPACES BETWEEN THEM\n",
    "    \"\"\"\n",
    "    in_str = str(in_str)\n",
    "    in_str = re.sub('@\\w+', '@MEN', in_str,flags=re.IGNORECASE) # use @\\w+ for word replacement or @ with space after @MEN for keeping mention\n",
    "    in_str = re.sub('#', '#HAS ', in_str,flags=re.IGNORECASE)\n",
    "    in_str = re.sub(r'([\\w])\\1+', r'\\1\\1', in_str) #reduce repeated characters to 2\n",
    "    return in_str.strip()\n",
    "\n",
    "\n",
    "def _simplify_punctuation(text):\n",
    "    \"\"\"\n",
    "    puntuations like '!!!!!' will be transformed into '!! <-EMPP'\n",
    "    This function simplifies doubled or more complex punctuation. The exception is '...'. #?! ??? !!!\n",
    "    \"\"\"\n",
    "    corrected = str(text)\n",
    "    corrected = re.sub(r'([!?,;])\\1+', r'\\1\\1 <-EMPP', corrected) #\\1\\1 makes it to 2 consecutive punctuation\n",
    "    corrected = re.sub(r'\\.{2,}', r'...', corrected)\n",
    "    return corrected\n",
    "\n",
    "def _reduce_repetitions(text):\n",
    "    \"\"\"\n",
    "    Auxiliary function to help with exxagerated (repeated characters in) words.\n",
    "    Examples:\n",
    "        woooooords -> woords <-EMPW\n",
    "        dooorwaaay -> doorwaay <-EMPW\n",
    "        SICK -> sick <-EMPU\n",
    "    \"\"\"\n",
    "    correction = str(text)\n",
    "    for index, words in enumerate(str(text).split()):\n",
    "        if _is_EMP_word(words)==True :\n",
    "            #insert EMPW after word\n",
    "            correction = correction.replace(words, words + ' <-EMPW')\n",
    "        if (len(words) > 4) & (words.isupper()==True) & (words[0] not in string.punctuation):\n",
    "            correction = correction.replace(words, words + ' <-EMPU')\n",
    "    #TODO work on complexity reduction.\n",
    "    return re.sub(r'([\\w])\\1+', r'\\1\\1', correction) #\\1\\1 will only keep 2 consecutive characters\n",
    "\n",
    "\n",
    "def _is_EMP_word(word):\n",
    "    \"\"\"\n",
    "    True/ False: checks if the word has 3 consecutive characters\"\"\"\n",
    "    count=1\n",
    "    if len(word)>1:\n",
    "        for i in range(1,len(word)):\n",
    "            if word[i] in string.punctuation: #this function is only for words!\n",
    "                return False\n",
    "            if word[i-1]==word[i]:\n",
    "                count+=1\n",
    "                if(count>=3):\n",
    "                     return True\n",
    "            else :\n",
    "                if(count>=3):\n",
    "                    return True\n",
    "                count=1\n",
    "    else :\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "\n",
    "def _normalize_whitespace(text):\n",
    "    \"\"\"\n",
    "    normalizes whitespaces, removing duplicates.\n",
    "    \"\"\"\n",
    "    corrected = str(text)\n",
    "    corrected = re.sub(r\"//t\",r\"\\t\", corrected)\n",
    "    corrected = re.sub(r\"( )\\1+\",r\"\\1\", corrected)\n",
    "    corrected = re.sub(r\"(\\n)\\1+\",r\"\\1\", corrected)\n",
    "    corrected = re.sub(r\"(\\r)\\1+\",r\"\\1\", corrected)\n",
    "    corrected = re.sub(r\"(\\t)\\1+\",r\"\\1\", corrected)\n",
    "    return corrected.strip(\" \")\n",
    "    \n",
    "    \n",
    "#Substitution of contractions:  -----------------------------------------------------------------------------------------------      \n",
    "def _normalize_contractions(sentence_list):\n",
    "    \"\"\"\n",
    "    it will correct each word in a sentence for slangs(ginger), emojis -> meaning, entity references and abbreviations(json file) : file can be manually modified above\n",
    "    also makes everything lowercase (including EMPW,EMPU, EMPP, URL, etc)\n",
    "    \"\"\"\n",
    "    #uses contraction_list (a json file) BE SURE TO IMPORT IT ALREADY\n",
    "    norm_sents = []\n",
    "    print(\"Normalizing contractions, abbreviations, slangs, emojis, character entities\")\n",
    "    for sentence in tqdm(sentence_list):\n",
    "        norm_sents.append(_normalize_contractions_slang_emoji_entity(sentence))\n",
    "    return norm_sents\n",
    "\n",
    "def _normalize_contractions_slang_emoji_entity(text):\n",
    "    \"\"\"\n",
    "    part1:normalizes english contractions.\n",
    "    \"\"\"\n",
    "    contractions = contraction_list\n",
    "    for word in text.split():\n",
    "         if word.lower() in contractions:\n",
    "            text = text.replace(word, contractions[word.lower()])\n",
    "    \"\"\"\n",
    "    part 2: using gingerit SMS slang correction:\n",
    "    this is too slow and can take many hours for the whole dataset to run\n",
    "    \"\"\"\n",
    "    sentence = text\n",
    "    \"\"\"\n",
    "    part3: emoji and character entity reference conversion to meaning\n",
    "    \"\"\"\n",
    "    emoticons = emot_obj.emoticons(sentence)\n",
    "    for i in range(0,len(emoticons['value'])):\n",
    "        sentence = sentence.replace(emoticons['value'][i], emoticons['mean'][i])\n",
    "    \"\"\"\n",
    "    part4: make everything lowercase\n",
    "    \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "206770eb-561c-49e5-b322-330a4b12ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_list = json.loads(open(\"english_contractions.json.txt\", 'r').read())\n",
    "character_entity= {'&lt;3':'heart', '&amp;':'and','&quot;':' quote '}\n",
    "contraction_list = {**contraction_list, **character_entity}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b160a4cf-ad60-4306-94db-89ed188c50be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "Starting Normalization Process\n",
      "Replacing -URL- , Replacing @MENTION and #HASHTAG, Reducing character repetitions, \n",
      "Simplifying punctuation, Removing whitespaces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 6662.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing contractions, abbreviations, slangs, emojis, character entities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization Process Finished\n",
      "##############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#assessment and examples:\n",
    "# original_examples = ['hi @someone WATCH me #proud :) ;) ...... i h8 it bt w8 !!!!!  <3  wanna go &amp; &lt;3 tHeRe  &quot; bcs my finls clooooose &quot;bananas&quot; &amp; ']\n",
    "original_examples=df.content[0:10]\n",
    "preprocessed_examples = normalization_pipeline(original_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7519a034-a101-4ea9-b0eb-1d9e646f4788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layin n bed with a headache  ughhhh...waitin on your call...\n",
      "layin n bed with a headache ughh...waitin on your call...\n",
      "Funeral ceremony...gloomy friday...\n",
      "funeral ceremony...gloomy friday...\n",
      "@dannycastillo We want to trade with someone who has Houston tickets, but no one will.\n",
      "@men we want to trade with someone who has houston tickets, but no one will.\n",
      "Re-pinging @ghostridah14: why didn't you go to prom? BC my bf didn't like my friends\n",
      "re-pinging @men: why did not you go to prom? bc my bf did not like my friends\n",
      "I should be sleep, but im not! thinking about an old friend who I want. but he's married now. damn, &amp; he wants me 2! scandalous!\n",
      "i should be sleep, but i am not! thinking about an old friend who i want. but he has married now. damn, and he wants me 2! scandalous!\n",
      "Hmmm. http://www.djhero.com/ is down\n",
      "hmm. -url- is down\n",
      "@charviray Charlene my love. I miss you\n",
      "@men charlene my love. i miss you\n",
      "@kelcouch I'm sorry  at least it's Friday?\n",
      "@men i am sorry at least it is friday?\n",
      "cant fall asleep\n",
      "cannot fall asleep\n",
      "Choked on her retainers\n",
      "choked on her retainers\n",
      "##############################\n",
      "Starting Normalization Process\n",
      "Replacing -URL- , Replacing @MENTION and #HASHTAG, Reducing character repetitions, \n",
      "Simplifying punctuation, Removing whitespaces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 31313/31313 [00:01<00:00, 28351.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing contractions, abbreviations, slangs, emojis, character entities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 31313/31313 [00:00<00:00, 237017.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization Process Finished\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "for example_index,example in enumerate(preprocessed_examples):\n",
    "#     print(original_examples[example_index])\n",
    "    print(original_examples.values[example_index])\n",
    "    print(example)\n",
    "\n",
    "    \n",
    "#run preprocessing\n",
    "df_original=df\n",
    "df.content=normalization_pipeline(df.content.values ) #about 10 minutes to run\n",
    "\n",
    "#save\n",
    "df.to_csv('df_processed.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d637ec6-37ef-411f-b6b9-6162162e2620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "df = pd.read_csv(r\"english_contractions.json.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a66f3fa1-fa84-43e9-a947-f1444976edc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         tweet_id  sentiment  \\\n",
      "0      1956967666    sadness   \n",
      "1      1956967696    sadness   \n",
      "2      1956968416    neutral   \n",
      "3      1956968477      worry   \n",
      "4      1956968487    sadness   \n",
      "...           ...        ...   \n",
      "31308  1753918954    neutral   \n",
      "31309  1753919001       love   \n",
      "31310  1753919005       love   \n",
      "31311  1753919043  happiness   \n",
      "31312  1753919049       love   \n",
      "\n",
      "                                                 content  label data_type  \n",
      "0      layin n bed with a headache ughh...waitin on y...      0     train  \n",
      "1                    funeral ceremony...gloomy friday...      0     train  \n",
      "2      @men we want to trade with someone who has hou...      1     train  \n",
      "3      re-pinging @men: why did not you go to prom? b...      2     train  \n",
      "4      i should be sleep, but i am not! thinking abou...      0     train  \n",
      "...                                                  ...    ...       ...  \n",
      "31308                                               @men      1     train  \n",
      "31309                      happy mothers day all my love      3     train  \n",
      "31310  happy mother's day to all the mommies out ther...      3     train  \n",
      "31311  @men wassup <-empu beautiful!! <-empu <-empp f...      4     train  \n",
      "31312  @men bullet train from tokyo the gf and i have...      3     train  \n",
      "\n",
      "[31313 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "df_processed = pd.read_csv(r\"df_processed.csv\")\n",
    "print(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "576dcf5f-7a36-40bf-ad6c-2145f1052a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x16ca13def40>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecf25b9b-9e70-4ffb-b84d-949dc6be7fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df_processed)\n",
    "dataset = dataset.rename_column('content', \"text\")\n",
    "dataset = dataset.rename_column('tweet_id', \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f158fa87-111a-4171-9ce4-a4030c430317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (4.21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\10450\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.8.1)\n",
      "Requirement already satisfied: requests in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from requests->transformers) (3.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\10450\\anaconda3\\envs\\e3nn3\\lib\\site-packages (from requests->transformers) (1.26.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  9.55ba/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#Loads model and tokenizes stuff\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\") #Replace \"\" with relevant model\n",
    "\n",
    "#Defines function for pre-processing - e.g. tokenising and truncating long phrases\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "#Maps pre-processing function to all of dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98e91d1f-ea2f-42c7-b3e8-6c30e450ab49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1956968416, 'sentiment': 'neutral', 'text': '@men we want to trade with someone who has houston tickets, but no one will.', 'label': 1, 'data_type': 'train'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12f525b2-7ee0-48cb-8690-6a9ad9377740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_accuracy(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a125f815-1db8-4823-b8c1-c7414ed20ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [00:01<00:00, 30.83ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00<00:00, 35.94ba/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\", num_labels=5) \n",
    "#Change model names and number of classes as is relevant.\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    #gradient_accumulation_steps=2,\n",
    "    #gradient_checkpointing=True,\n",
    "    #Memory-saving, more efficient way of reviewing dataset entries. Multiply by batch size for effective batch size.\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32, #Decrease to save memory, but start at 16\n",
    "    per_device_eval_batch_size=32, #Decrease to save memory, but start at 16\n",
    "    optim=\"adafactor\", #Optimiser - add to save memory\n",
    "    num_train_epochs=6, #Start at five, but alter based on optimum loss\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset.filter(lambda x: x['data_type'] =='train'),\n",
    "    eval_dataset=tokenized_dataset.filter(lambda x: x['data_type'] =='val'),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_accuracy, #Call accuracy score function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f68b032-11b5-4032-8a65-8bf9e0a4f4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1957019458, 'sentiment': 'neutral', 'text': '#has 3wordsaftersex it never started...', 'label': 1, 'data_type': 'train', 'input_ids': [0, 85, 90, 698, 25844, 37734, 1871, 557, 18, 143, 4469, 13978, 28, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64e4bca3-00b6-487b-936c-eaed158736f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: data_type, sentiment, id, text. If data_type, sentiment, id, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 30530\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5730\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5730' max='5730' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5730/5730 19:36, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.265900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.194100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.097700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.077800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.937100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.894000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.769600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.696300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.589700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.536400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.453300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-500\\special_tokens_map.json\n",
      "added tokens file saved in ./results\\checkpoint-500\\added_tokens.json\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1000\\special_tokens_map.json\n",
      "added tokens file saved in ./results\\checkpoint-1000\\added_tokens.json\n",
      "Saving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-1500\\special_tokens_map.json\n",
      "added tokens file saved in ./results\\checkpoint-1500\\added_tokens.json\n",
      "Saving model checkpoint to ./results\\checkpoint-2000\n",
      "Configuration saved in ./results\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-2000\\special_tokens_map.json\n",
      "added tokens file saved in ./results\\checkpoint-2000\\added_tokens.json\n",
      "Saving model checkpoint to ./results\\checkpoint-2500\n",
      "Configuration saved in ./results\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-2500\\special_tokens_map.json\n",
      "added tokens file saved in ./results\\checkpoint-2500\\added_tokens.json\n",
      "Saving model checkpoint to ./results\\checkpoint-3000\n",
      "Configuration saved in ./results\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-3000\\special_tokens_map.json\n",
      "added tokens file saved in ./results\\checkpoint-3000\\added_tokens.json\n",
      "Saving model checkpoint to ./results\\checkpoint-3500\n",
      "Configuration saved in ./results\\checkpoint-3500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-3500\\special_tokens_map.json\n",
      "added tokens file saved in ./results\\checkpoint-3500\\added_tokens.json\n",
      "Saving model checkpoint to ./results\\checkpoint-4000\n",
      "Configuration saved in ./results\\checkpoint-4000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-4000\\special_tokens_map.json\n",
      "added tokens file saved in ./results\\checkpoint-4000\\added_tokens.json\n",
      "Saving model checkpoint to ./results\\checkpoint-4500\n",
      "Configuration saved in ./results\\checkpoint-4500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-4500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-4500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-4500\\special_tokens_map.json\n",
      "added tokens file saved in ./results\\checkpoint-4500\\added_tokens.json\n",
      "Saving model checkpoint to ./results\\checkpoint-5000\n",
      "Configuration saved in ./results\\checkpoint-5000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-5000\\special_tokens_map.json\n",
      "added tokens file saved in ./results\\checkpoint-5000\\added_tokens.json\n",
      "Saving model checkpoint to ./results\\checkpoint-5500\n",
      "Configuration saved in ./results\\checkpoint-5500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-5500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-5500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-5500\\special_tokens_map.json\n",
      "added tokens file saved in ./results\\checkpoint-5500\\added_tokens.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5730, training_loss=0.848871063978052, metrics={'train_runtime': 1181.3175, 'train_samples_per_second': 155.064, 'train_steps_per_second': 4.851, 'total_flos': 4044608918136360.0, 'train_loss': 0.848871063978052, 'epoch': 6.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aef64193-93f6-4d8f-8bd8-213fb8cefa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'original_config.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bb90749-206e-4339-8def-bac29d77e3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('original_config.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eacef2f8-33d6-4307-9e7e-895f9bd6dc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text, sentiment, id, data_type. If text, sentiment, id, data_type are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 783\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.32028037309646606,\n",
       " 'eval_accuracy': 0.9003831417624522,\n",
       " 'eval_runtime': 1.8566,\n",
       " 'eval_samples_per_second': 421.74,\n",
       " 'eval_steps_per_second': 13.466}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd708985-41da-4cde-920b-14b04b55c1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6687, -0.3363,  3.4640, -1.7952, -3.3844]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "test = {'text': ['''\n",
    "\n",
    "modern players are bad enough\n",
    "at least i can focus on the game\n",
    "commander you can't\n",
    "because it's all social politics\n",
    "no actual gameplay\n",
    "\n",
    "''']}\n",
    "\n",
    "test = preprocess_function(test)\n",
    "\n",
    "test['input_ids'] = torch.tensor(test['input_ids'])\n",
    "test['attention_mask'] = torch.tensor(test['attention_mask'])\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "test['input_ids'] = test['input_ids'].to(device) \n",
    "test['attention_mask'] = test['attention_mask'].to(device)\n",
    "\n",
    "print(\"predictions:\", model(test['input_ids'], test['attention_mask']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf86a52-1cd1-4f02-921e-e93e29541913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'sadness': 0, 'neutral': 1, 'worry': 2, 'love': 3, 'happiness': 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
